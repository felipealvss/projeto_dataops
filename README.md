# ğŸš€ **Projeto DataOps Unifor**

**DataOps Unifor** Ã© uma soluÃ§Ã£o de engenharia de dados de ponta, que orquestra pipelines de dados, realiza transformaÃ§Ãµes em tempo real e oferece visualizaÃ§Ãµes dinÃ¢micas e interativas. O projeto integra vÃ¡rias tecnologias inovadoras para criar um fluxo de trabalho completo e automatizado de dados.

## ğŸ¯ **Objetivo do Projeto**

O objetivo principal Ã© automatizar o fluxo de dados atravÃ©s de pipelines orquestrados pelo **Apache Airflow**, utilizando containers **Docker** para isolar os serviÃ§os e garantir escalabilidade e eficiÃªncia. A soluÃ§Ã£o final inclui:

* **OrquestraÃ§Ã£o de Dados**: Gerenciamento de pipelines e execuÃ§Ã£o automÃ¡tica de tarefas.
* **TransformaÃ§Ãµes em Tempo Real**: Processamento de dados Ã  medida que chegam.
* **VisualizaÃ§Ã£o de Dados**: Dashboards interativos para insights rÃ¡pidos e fÃ¡ceis.

---

## ğŸ›  **Tecnologias Utilizadas**

Este projeto utiliza um conjunto robusto de tecnologias para garantir a automaÃ§Ã£o, escalabilidade e facilidade de uso:

* **Apache Airflow**: OrquestraÃ§Ã£o de workflows e agendamento de tarefas.
* **Docker**: ContainerizaÃ§Ã£o dos serviÃ§os para maior flexibilidade e isolamento.
* **PostgreSQL**: Banco de dados relacional para persistÃªncia de dados estruturados.
* **MongoDB Atlas**: Banco de dados NoSQL para dados nÃ£o estruturados.
* **FastAPI**: Framework rÃ¡pido e moderno para a construÃ§Ã£o de APIs RESTful.
* **Streamlit**: Framework para criaÃ§Ã£o de dashboards interativos.
* **Redis**: Sistema de gerenciamento de cache e filas.
* **Poetry**: Gerenciamento eficiente de dependÃªncias Python.

---

## ğŸ—‚ **Estrutura do Projeto**

A estrutura do projeto foi organizada para ser intuitiva e modular, facilitando a navegaÃ§Ã£o e o desenvolvimento.

```plaintext
.
â”œâ”€â”€ config                       # Arquivos de configuraÃ§Ã£o do Airflow
â”œâ”€â”€ dags                         # DAGs do Airflow para orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ dag_gerar_dados.py       # GeraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ dag_vendas_ano_mes.py    # Processamento de vendas por ano e mÃªs
â”‚   â”œâ”€â”€ dag_vendas_estado.py     # Processamento de vendas por estado
â”‚   â””â”€â”€ dag_vendas_modalidade.py # Processamento de vendas por modalidade
â”œâ”€â”€ dataops_unifor               # MÃ³dulo principal
â”œâ”€â”€ docker                       # ConfiguraÃ§Ãµes Docker para FastAPI e Streamlit
â”œâ”€â”€ docker-compose.yaml          # OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ docs                         # DocumentaÃ§Ã£o adicional
â”œâ”€â”€ dump                         # Dumps de banco de dados
â”œâ”€â”€ logs                         # Logs do Airflow
â”œâ”€â”€ plugins                      # Plugins customizados do Airflow
â”œâ”€â”€ poetry.lock                  # Bloqueio de dependÃªncias
â”œâ”€â”€ pyproject.toml               # ConfiguraÃ§Ã£o do Poetry
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ src                          # CÃ³digo-fonte do projeto
â”‚   â”œâ”€â”€ fastapi_app              # AplicaÃ§Ã£o FastAPI
â”‚   â””â”€â”€ streamlit_dashboard      # Dashboard Streamlit
â””â”€â”€ tests                        # Testes do projeto
```

---

## ğŸ“ **Como Rodar o Projeto**

### 1. **Instalar DependÃªncias**

Certifique-se de que o **Docker** e o **Docker Compose** estÃ£o instalados. Se nÃ£o, instale-os [aqui](https://www.docker.com/get-started).

Clone o repositÃ³rio e instale as dependÃªncias com o **Poetry**:

```bash
git clone https://github.com/seu-usuario/dataops_unifor.git
cd dataops_unifor
poetry install
```

### 2. **Subir os Containers Docker**

Suba todos os containers definidos no arquivo `docker-compose.yaml`:

```bash
docker-compose up --build
```

Isso irÃ¡ iniciar os seguintes serviÃ§os:

* **PostgreSQL**: Banco de dados relacional.
* **Redis**: Gerenciador de filas para o Airflow.
* **Airflow**: OrquestraÃ§Ã£o de tarefas (DAGs).
* **FastAPI**: API backend.
* **Streamlit**: Dashboard interativo.

### 3. **Acessar os ServiÃ§os**

* **Airflow Web UI**: [http://localhost:8080](http://localhost:8080)
* **FastAPI**: [http://localhost:8000](http://localhost:8000)
* **Streamlit**: [http://localhost:8501](http://localhost:8501)

### 4. **Executar os DAGs**

Os DAGs podem ser visualizados e executados atravÃ©s da interface web do Airflow. Os DAGs disponÃ­veis sÃ£o:

* **dag\_gerar\_dados.py**: GeraÃ§Ã£o e ingestÃ£o de dados.
* **dag\_vendas\_ano\_mes.py**: Processamento de vendas por ano e mÃªs.
* **dag\_vendas\_estado.py**: Processamento de vendas por estado.
* **dag\_vendas\_modalidade.py**: Processamento de vendas por modalidade.

---

## âš™ **Estrutura do `docker-compose.yaml`**

Este arquivo orquestra os serviÃ§os Docker. Ele inclui:

* **Airflow**: ConfiguraÃ§Ã£o dos containers para o `webserver`, `scheduler`, `worker`, `dag-processor`, e `triggerer`.
* **PostgreSQL**: Banco de dados relacional utilizado pelo Airflow.
* **Redis**: Broker de filas para o Airflow.
* **FastAPI e Streamlit**: Containers para o backend e visualizaÃ§Ã£o de dados.

---

## ğŸ§ª **Como Testar**

O projeto inclui testes automatizados para garantir a qualidade do cÃ³digo. Para rodÃ¡-los, basta utilizar o comando:

```bash
python tests/test_api.py
```

O arquivo `test_api.py` contÃ©m testes para garantir que a API **FastAPI** estÃ¡ funcionando corretamente.

---

## ğŸ¤ **ContribuiÃ§Ã£o**

ContribuiÃ§Ãµes sÃ£o sempre bem-vindas! Para contribuir:

1. FaÃ§a um **fork** deste repositÃ³rio.
2. Crie uma nova branch para sua feature:
   `git checkout -b feature/nome-da-feature`
3. FaÃ§a as alteraÃ§Ãµes e adicione um commit:
   `git commit -am 'Adicionando nova feature'`
4. FaÃ§a o push para sua branch:
   `git push origin feature/nome-da-feature`
5. Abra um **Pull Request** explicando as mudanÃ§as.

---

## ğŸ“„ **LicenÃ§a**

Este projeto estÃ¡ licenciado sob a **MIT License**. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---
